# Accelerated-Loan-Risk-Assesement
Using a spark notebook to accelerate loan default model creation.

### Background:
Attemping to predict loan defaults is one of the many use cases for machine learning in the finacial services industry.
Datasets generated by banks are massive and lead to long model training times and can lead to slower decisions which isn't great when you're trying to apply for a loan.  For most problems, Python + Pandas are sophisticated enough to handle the workload but as file sizes get larger, knowing how to program in Spark is increasingly important. 

### Solution:
I've created a simple spark scala script to handle the most computationally inenstive parts of training the loan default classification model that can be applied to a dataset that I preprocessed from Lending Club in Jupyter Notebook.
<br>
- Scala is the language that will perform best under most circumstances.  Spark was originally written in Scala so it stands to reason that Scala scripts will run faster.
- PySpark is also relatively fast and has more packages in its library, such as SparkSQL and a package to convert your data to Pandas.

### Data Source:
[Data pulled from the Lending Club Kaggle Chanllege](https://www.kaggle.com/wendykan/lending-club-loan-data)
<br>
Lending Club is a website that provides [personal loans](https://www.lendingclub.com/investing/investor-education/what-is-a-lendingclub-note) to borrowers and packages them up for investors to buy. This process is known as securitization, a process that relies on the investor knowing weather or not a borrowers is likely to default.  A more refined version of this model could be used to rate securities for prospective investors.
